{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d48ee7-36c3-4107-aef6-faa1c294cfb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Run this cell once without comments to install packages, then comment them out.\n",
    "#!pip install openai\n",
    "#!pip install langchain\n",
    "#!pip install tiktoken\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce725716-8f58-4991-8838-b3d07e36b730",
   "metadata": {},
   "source": [
    "# Don't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35ecada-3ca2-4a68-9b74-e72cbad23437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI, ChatVertexAI\n",
    "from langchain.embeddings import OpenAIEmbeddings, VertexAIEmbeddings\n",
    "from langchain.schema import (\n",
    "    AIMessage, \n",
    "    HumanMessage, \n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"be9bdecc8bf64e85bde69c04b2ad56f8\"\n",
    "os.environ[\"OPENAI_API_HOST\"] = \"https://tiaa-openai-azure-sweden.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_EMBEDDING_KEY\"] = \"be9bdecc8bf64e85bde69c04b2ad56f8\"\n",
    "os.environ[\"OPENAI_API_EMBEDDING_HOST\"] = \"https://tiaa-openai-azure-sweden.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] =  \"2023-07-01-preview\"\n",
    "import openai\n",
    "openai.api_type='azure'\n",
    "openai.api_base = os.environ[\"OPENAI_API_HOST\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "#model_name='tiaa-gpt-4-32k'\n",
    "base_model_name=\"tiaa-gpt-4-32k\"\n",
    "chosen_llm = AzureChatOpenAI(\n",
    "        openai_api_base=os.environ[\"OPENAI_API_HOST\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "        deployment_name=base_model_name,\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        openai_api_type=\"azure\",\n",
    "        temperature=0\n",
    "    )\n",
    "embedding_model_name = 'tiaa-text-embedding-ada-002'\n",
    "chosen_embedding_model = OpenAIEmbeddings(\n",
    "        deployment=embedding_model_name,\n",
    "        openai_api_base=os.environ[\"OPENAI_API_EMBEDDING_HOST\"],\n",
    "        openai_api_version= os.environ[\"OPENAI_API_VERSION\"],\n",
    "        openai_api_key=os.environ[\"OPENAI_API_EMBEDDING_KEY\"],\n",
    "        openai_api_type=\"azure\",\n",
    "        chunk_size=16\n",
    "    )\n",
    "finalAnswerTheQuestionModel = AzureChatOpenAI(\n",
    "        openai_api_base=os.environ[\"OPENAI_API_HOST\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "        deployment_name=base_model_name,\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        openai_api_type=\"azure\",\n",
    "        temperature=0,\n",
    "        verbose = True\n",
    "    )\n",
    "condensingTheQuestionModel = AzureChatOpenAI(\n",
    "        openai_api_base=os.environ[\"OPENAI_API_HOST\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "        deployment_name=base_model_name,\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        openai_api_type=\"azure\",\n",
    "        temperature=0, \n",
    "        verbose = True\n",
    "    )\n",
    "memory = ConversationBufferMemory(\n",
    "    output_key='answer',\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef1291-5506-4a32-a84b-321c8ff29678",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Asking general questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6f4b946-f4d2-4cbf-9af6-eb04082d5c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris and the capital of England is London. London is a bigger city in terms of both population and area.\n"
     ]
    }
   ],
   "source": [
    "msg = HumanMessage(content=\"Where are capitals of France and England? Which one is a bigger city\")\n",
    "output=chosen_llm(messages=[msg])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c356a-8795-49e3-b521-208ed2f7e510",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d598c6e7-98ab-474a-a1ab-58a3a9264077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "text2=f\"\"\"hello world!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc356c-8fde-4aa5-9d92-8fb7cded6515",
   "metadata": {},
   "source": [
    "## Asking questions from the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c2cb952-37b2-4403-bc33-53d38ba82bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic discussed in the text is the importance of providing clear and specific instructions to a model to guide it towards the desired output and reduce the chances of irrelevant or incorrect responses.\n",
      "There are 12 characters in the text \"hello world!\".\n"
     ]
    }
   ],
   "source": [
    "msg = HumanMessage(content=f\" What is the main  topic discussed in the text?  ```{text}```\")\n",
    "output=chosen_llm(messages=[msg])\n",
    "print(output.content)\n",
    "msg = HumanMessage(content=f\" How many characters are there in the text?  ```{text2}```\")\n",
    "output=chosen_llm(messages=[msg])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0556b7-fd58-40d7-a631-597d2d2fd4b5",
   "metadata": {},
   "source": [
    "## Embedding a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ccb7bb8-dd4e-47be-afcf-629974f93e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorstore(text):\n",
    "    new_doc = Document(page_content=text, metadata=[])\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunked_docs = text_splitter.split_documents([new_doc])\n",
    "    vectorstore = FAISS.from_documents(chunked_docs, chosen_embedding_model)\n",
    "    return(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7eaf9e0-e650-4f99-8488-cae78362bded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_vector=vectorstore(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cd513-e09c-4569-8b56-96e38d3573dc",
   "metadata": {},
   "source": [
    "## Asking questions from the embedded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b17d1ca8-da33-4ac4-a855-9c702f2e9215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "\n",
    "    llm = finalAnswerTheQuestionModel,\n",
    "\n",
    "    retriever = current_vector.as_retriever(),\n",
    "\n",
    "    condense_question_llm = condensingTheQuestionModel,\n",
    "\n",
    ")\n",
    "\n",
    "source_with_qa = ConversationalRetrievalChain.from_llm(\n",
    "\n",
    "    llm = finalAnswerTheQuestionModel,\n",
    "\n",
    "    retriever = current_vector.as_retriever(\n",
    "\n",
    "                                         search_kwargs={\n",
    "\n",
    "                                            \"k\": 8 #num_nearest_neighbors.value,\n",
    "\n",
    "                                            }),\n",
    "\n",
    "    condense_question_llm = condensingTheQuestionModel,\n",
    "\n",
    "    memory=memory,\n",
    "\n",
    "    return_source_documents=True,\n",
    "\n",
    "    verbose=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cdf7721-3edb-45be-8a6c-a3a07780aaac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main topic discussed in this document is the importance of providing clear and specific instructions to a model to guide it towards the desired output and reduce the chances of receiving irrelevant or incorrect responses. It also emphasizes that longer prompts can often provide more clarity and context for the model.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = f\" What is the main  topic discussed in this document?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b2234-b712-4eff-97d2-cbca83565f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
